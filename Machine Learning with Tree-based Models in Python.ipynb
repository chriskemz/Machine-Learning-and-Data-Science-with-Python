{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ccb641-2171-4767-853a-333d8ac5e368",
   "metadata": {},
   "source": [
    "## Decision tree for classification\n",
    "\n",
    "__Classification Tree__ \\\n",
    "A Classification Tree is a supervised learning model that predicts a categorical label (like 'Yes'/'No' or 'Spam'/'Not Spam') by learning a series of simple if/else decision rules from the data. The best way to think about it is as an automated flowchart. The model asks a sequence of questions to the data, and each answer leads it down a different path until it reaches a final decision.\n",
    "\n",
    "Imagine you're playing a game where your friend is thinking of an animal.\n",
    "\n",
    "* You ask: \"Does it live in water?\" (This is your first split)\n",
    "\n",
    "* Answer: Yes. -> You then ask: \"Does it have fins?\"\n",
    "\n",
    "* Answer: No. -> You then ask: \"Is it a mammal?\" \\\n",
    "You continue asking these simple questions to narrow down the possibilities until you can make a final guess. A classification tree learns the most efficient sequence of questions to ask to classify your data.\n",
    "\n",
    "In contrast to linear models, trees are able to capture non-linear relationships between features and labels. In addition, trees do not require the features to be on the same scale, for example through standardization.\n",
    "\n",
    "__How Does a Classification Tree Work?__ \\\n",
    "The \"magic\" of a decision tree is how it learns which questions to ask and in what order. It does this by finding the splits that create the \"purest\" possible subgroups.\n",
    "\n",
    "1. __Start with all data__: The tree starts with all your data in one group at the top (the root node).\n",
    "\n",
    "2. __Find the Best Split__: The algorithm looks at every single feature and finds the one question (the best split) that does the best job of separating the data into distinct classes. For example, it might find that asking \"Is the weather outlook sunny?\" is the best first question because it perfectly separates many of the \"Go to Beach\" vs. \"Stay Home\" outcomes.\n",
    "\n",
    "3. __Measure \"Purity\"__: It determines the \"best\" split using a metric like Gini Impurity. Gini impurity is a measure of how mixed up the classes are in a group. A Gini of 0 means a group is perfectly pure (all one class). The algorithm chooses the split that leads to the biggest decrease in impurity.\n",
    "\n",
    "4. __Repeat__: The process is repeated for each new subgroup. The tree continues to split the data into smaller and smaller, purer and purer groups until it reaches a stopping point (e.g., a group is perfectly pure, or the tree reaches a pre-defined maximum depth).\n",
    "\n",
    "The maximum number of branches separating the top from an exreme end is known as the __maximum depth__. \n",
    "\n",
    "__Decision region__ \\\n",
    "A decision region is a region in the feature space where all instances in one region are assigned only to one class label. These regions are separated by surfaces called __Decision boundaries__. The decision boundary produced by logistic regression is linear while the boundaries produced by the classification tree divide the feature space into rectangular regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d61e390-e624-4838-9c7c-e2d04109550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  diagnosis  radius_mean  concave points_mean\n",
      "0         M        17.99              0.14710\n",
      "1         M        20.57              0.07017\n",
      "2         M        19.69              0.12790\n",
      "3         M        11.42              0.10520\n",
      "4         M        20.29              0.10430\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wbc = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\wbc.csv\", usecols=['diagnosis', 'radius_mean', 'concave points_mean'])\n",
    "print(wbc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2499f66d-3088-4021-b20f-b9ed7daf381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0]\n",
      "Test set accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = wbc[['radius_mean', 'concave points_mean']].values\n",
    "y = np.where(wbc['diagnosis'] == 'M', 1, 0)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y, random_state=1)\n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec1b72-0585-4237-a09c-99dcdfbd6525",
   "metadata": {},
   "source": [
    "## Classification tree learning\n",
    "\n",
    "A classification tree learns by creating a set of decision rules, much like a flowchart, to predict a categorical outcome. It does this by recursively splitting the data into smaller, purer subgroups based on the features that provide the most __Information Gain__.\n",
    "\n",
    "__Building blocks of a Decision-Tree__ \\\n",
    "1. __Decision-Tree__: A decision-tree is a data structure consisting of a hierarchy of individual units called __nodes__.\n",
    "2. __Node__: A node is a point that involves either a question or a prediction.\n",
    "   * __Root node__: This is the node at which the decision tree starts growing. It has no parent node and involves a question that gives rise to two children nodes through two branches.\n",
    "   * __Internal node__: This is a node that have one parent node. It also involves a question that gives rise to two children nodes.\n",
    "   * __Leaf__: This node has no children. A leaf has one parent node and involves no questions. It is where a prediction is made.\n",
    "\n",
    "Recall that when a classification tree is trained on a labeled dataset, the tree learns patterns from the features in such a way to produce the purest leafs. In other words, the tree is trained in such a way that, in each leaf, one class-label is predominant.\n",
    "\n",
    "__Information Gain (IG)__ \\\n",
    "The nodes of a classification tree are grown recursively; in other words, the obtention of an internal node or a leaf depends on the state of its predecessors. To produce the purest leafs possible, at each node, a tree asksa question involving one feature and one split-point. The tree knows the feature and split-point to pick by maximizing the Information Gain.\n",
    "\n",
    "To understand Information Gain, you first need to understand impurity. Impurity is a number that tells you how mixed a group is. Criteria used to measure the impurity of a node include:\n",
    "1. Gini Index\n",
    "2. Entropy\n",
    "\n",
    "* A Gini Impurity of 0 means a group is perfectly pure (all data points belong to one class).\n",
    "\n",
    "* A high Gini Impurity means a group is very mixed.\n",
    "\n",
    "Information Gain is the measure of the reduction in impurity achieved by a particular split. It's calculated as: \\\n",
    "IG = Impurity(parent node) - Weighted Average Impurity(child nodes)\n",
    "\n",
    "The tree learns by always choosing the split that maximizes this Information Gain. It's always looking for the question that will give it the most clarity and lead to the purest possible subgroups.\n",
    "\n",
    "When an unconstrained tree is trained, the nodes are grown recursively. In other words, a node exists based on the state of its predecessors. At a non-leaf node, the data is split based on feature and split-point in such a way to maximize Information Gain. If the information gain obtained by splitting a node is null, the node is declared a leaf. If the maximum depth of a tree is constrained, to 2 for example, all nodes having a depth of 2 will be declared leafs even if the information gain obtained by splitting such nodes is not null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a505d73e-8c13-47e3-8945-ea86eb65f2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy: 0.912\n"
     ]
    }
   ],
   "source": [
    "## Using entropy as criterion\n",
    "wbc_all = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\wbc.csv\")\n",
    "\n",
    "X = wbc_all.drop('diagnosis', axis=1).values\n",
    "y = np.where(wbc['diagnosis'] == 'M', 1, 0)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y, random_state=1)\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1237176a-deac-4f98-bafc-d8fa92176c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy: 0.939\n"
     ]
    }
   ],
   "source": [
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(criterion='gini', max_depth=8, random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred= dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_gini = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print(f'Accuracy achieved by using entropy: {accuracy_gini:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69313050-d336-4cea-9ee6-b759d8333fd4",
   "metadata": {},
   "source": [
    "## Decision-Tree for Regression\n",
    "\n",
    "A Regression Tree is a type of decision tree used to predict a continuous numerical value (like a price, an age, a temperature, etc.). Just like a classification tree, it creates a flowchart of if/else rules, but instead of ending in a class label ('Yes'/'No'), it ends in a numerical prediction. In classification tree, the goal is to create subgroups that are as \"pure\" as possible (containing mostly one class), in which case the prediction will be the majority in a leaf node. However, in regression tree, the goal is to create subgroups where the target values are as similar as possible to each other. The prediction is the average of the target values in a leaf node.\n",
    "\n",
    "Recall that linear models such as linear regression would not be able to capture a non-linear trend.\n",
    "\n",
    "When a regression tree is trained on a dataset, the impurity of a node is measured using the mean squares error of the target values in that node. This means that the regression tree try to find the splits that produce leafs where in each leaf, the target values are on average, the closest possible to the mean-value of the labels in that particular leaf. As a new instance traverses and reaches a certain leaf, its target variable y, is computed as the as the average of the target variables contained in that leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9ffd18-d446-49ff-8d04-7de3d5a5da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  displ   hp  weight  accel  origin  size\n",
      "0  18.0  250.0   88    3139   14.5      US  15.0\n",
      "1   9.0  304.0  193    4732   18.5      US  20.0\n",
      "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
      "3  18.5  250.0   98    3525   19.0      US  15.0\n",
      "4  34.3   97.0   78    2188   15.8  Europe  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "auto = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\auto.csv\")\n",
    "print(auto.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610114ae-a2ba-4cb6-9650-2035f919ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# perform one hot encoding on origin\n",
    "auto_encoded = pd.get_dummies(auto, columns=['origin'], drop_first=True)\n",
    "\n",
    "X = auto_encoded.drop('mpg', axis=1).values\n",
    "y = auto_encoded['mpg'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=3)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt ** (1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb402fbd-7169-4257-9c49-1ff488ff7740",
   "metadata": {},
   "source": [
    "## Generalization Error\n",
    "\n",
    "In supervised learning, you make the assumption that there is a mapping $f$ between features and labels. You can express this as $ y = f(x) $. $f$ is an unknown function you want to determine. In reality, data generalization is always accompanied with randomness or noise. Your goal is to find a model, $\\hat{f}$ that best approximates $f$. $\\hat{f}$ can be Logistic regression, Decision Tree, Neural Network, etc. When training  $\\hat{f}$, you want to make sure that noise is discarded as much as possible. At the end,  $\\hat{f}$ should achieve a low predictive error on unseen datasets.\n",
    "\n",
    "You may encounter two difficulties when approximating $f$:\n",
    "1. __Overfitting__: This when  $\\hat{f}$ fits the noise in the training set\n",
    "2. __Underfitting__: This is when  $\\hat{f}$ is not flexible enough to approximate f.\n",
    "\n",
    "When a model overfits the training the set, its predictive power on unseen datasets is pretty low. The model will memorize the noise present in the training set. Such model achieves a low training set error and a high test set error. When a model underfits the data, the training set error is roughly equal to the test set error. However, both errors are relatively high. As a result, the trained model is not flexible enough to capture the complex dependency between features and labels.\n",
    "\n",
    "The generalization error of a model is the measure of how accurately the model is able to predict outcomes for new, previously unseen data. It tells you how the model generalizes on unseen data\n",
    "\n",
    "__The Three Components of Generalization Error__ \\\n",
    "Generalization error can be decomposed into three components:\n",
    "1. __Bias error__: This tells you, on average, how $\\hat{f}$ and $f$ are different. A high-bias model fails to capture the true underlying patterns in the data. High bias models lead to underfitting.\n",
    "2. __Variance error__: This tells you how much $\\hat{f}$ is inconsistent over different training sets. A high-variance model learns the \"noise\" in the training data, not just the signal. A high variance model leads to overfitting\n",
    "3. __Irreducible error__: This is the error contribution of noise. It is the baseline error that you can never get rid of, no matter how good your model is. Thus, it is a constant.\n",
    "\n",
    "__Model Complexity__ \\\n",
    "The complexity of a model sets its flexibility to approximate the true function $f$. For example, increasing the maximum depth increases the complexity of a Decision-Tree.\n",
    "\n",
    "__Bias-Variance Tradeoff__\\\n",
    "When model complexity increases, the variance increases while the bias decreases and vice versa. Your goal is to find the model complexity that achieves the lowest generalization error. Thus, we need to find a balance between bias and variance because as one increases, the other decreases. This is knowns as the __bias-variance tradeoff__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a6619-731b-4f91-8907-d5bbcf05b404",
   "metadata": {},
   "source": [
    "## Diagnosing Bias and Variance Problems\n",
    "\n",
    "__Estimating the Generalization Error__ \\\n",
    "Given that you have trained a supervised learning model, $\\hat{f}$, how do you estimate $\\hat{f}$ generalization error? This cannot be done directly because:\n",
    "* $f$ is unknown\n",
    "* you usually have only one dataset\n",
    "* noise is unpredictable\n",
    "\n",
    "A solution to this is to:\n",
    "* split the data into a training and test set\n",
    "* fit the model $\\hat{f}$ to the training set\n",
    "* evaluate the error of $\\hat{f}$ on the unseen test set\n",
    "* approximate the generalization error of $\\hat{f}$ by the $\\hat{f}$ error on the test set\n",
    "\n",
    "__Better Model Evaluation with Cross-Validation__ \\\n",
    "Usually, the test set should not be touched until we are confident about $\\hat{f}$'s performance. It should only be used to evalute $\\hat{f}$'s final performance or error. Evaluating $\\hat{f}$ performance on the training set may produce an optimistic estimation of the error becahuse $\\hat{f}$ has already been exposed to the training set when it was fit. To obtain a reliable estimate of $\\hat{f}$'s performance, use a technique called __cross-validation__ (CV). CV can be performed using K-Fold CV or Hold-Out CV. Let's focus on K-Fold. Here, the training set is split randomly into a number of folds, say 10. The error of $\\hat{f}$ is evaluated ten times on the 10 folds. Each time, one fold is picked for evaluation after training $\\hat{f}$ on the other 9 folds. At the end, you will obtain a list of 10 errors. Finally, the CV-error is computed as the mean of the 10 obtained errors.\n",
    "\n",
    "__Diagnose Variance Problems__ \\\n",
    "If the CV-error of $\\hat{f}$ is greater than $\\hat{f}$'s training set error, $\\hat{f}$ is said to suffer from __high variance__. In such case, $\\hat{f}$ has overfit the training set. To remedy this, try decreasing the model complexity. For example, in a decision tree,you can reduce the maximum tree depth or increase the maximum samples per leaf. In addition, you may also gather more data to train $\\hat{f}$.\n",
    "\n",
    "__Diagnose Bias Problems__ \\\n",
    "$\\hat{f}$ is said to suffer from __high bias__ if the CV error is roughly equal to the training error but much greater than the desired error. In such case, $\\hat{f}$ underfits the training set. To remedy this, try increasing the model complexity or gather more relevant features for the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e66333-6f92-4a5c-b622-472b49444bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)\n",
    "\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3250ca3-8d64-4430-886f-93467b58b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the training error\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec933f71-fff5-4916-832c-d84e4b078d8d",
   "metadata": {},
   "source": [
    "Here,we notice how the training error is roughly equal to the 10-folds CV error you obtained in the previous exercise. Thus, we can conclude that the model suffers from high bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1df252-6807-4720-bf56-b3e6860b43cd",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "Ensemble learning is the technique of combining the predictions from multiple machine learning models to make a more accurate and robust prediction than any single model could on its own. It can be summarized as follows: \n",
    "1. Different models are trained on the same dataset\n",
    "2. Each model makes its own prediction\n",
    "3. A meta-model then aggregates the predictions of individual models and outputs a final prediction\n",
    "4. The final prediction is more robust and less prone to errors.\n",
    "5. The best results are obtained when the models are skillful in different ways, meaning that if some models make predictions that are way off, the other models should compensate these errors.\n",
    "\n",
    "There are few techniques in ensemble learning including:\n",
    "* voting\n",
    "* bagging\n",
    "* boosting\n",
    "\n",
    "We will focus on voting here.\n",
    "\n",
    "__Voting Classifier__ \\\n",
    "The ensemble here consists of N classifiers making predictions $P_1$, $P_2$, ..., $P_N$, with $P_i$ = 0 or 1. This is for a binary classification task. The meta-model outputs the final prediction by hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399ee7cf-4408-441a-bdff-b90a09ee78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_liver_patient = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\indian_liver_patient_preprocessed.csv\")\n",
    "\n",
    "X = indian_liver_patient.drop(\"Liver_disease\", axis=1)\n",
    "y = indian_liver_patient['Liver_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3239b53-1a7b-4777-a2e2-9c5ad05be891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNeighborsClassifier(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f4ddd2-fcbb-41d3-81b7-1071e0e2fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.747\n",
      "K Nearest Neighbours : 0.724\n",
      "Classification Tree : 0.730\n"
     ]
    }
   ],
   "source": [
    "## Evaluate different individual classifiers\n",
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35fbd4-f863-4462-a5a7-9ece0f4e3092",
   "metadata": {},
   "source": [
    "Logistic Regression achieved the highest accuracy of 74.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b75cae2-62f3-443d-84ae-5cc02d32c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.759\n"
     ]
    }
   ],
   "source": [
    "## evaluating performance using VotingClassifier\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8bcc3-7c24-48d6-a42e-63a36856f94f",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Previously, we saw that Voting Classifier is an ensemble of models that are fit to the same training set using different algorithms. We also saw that the final predictions were obtained by majority voting. In Bagging, the ensemble is formed by models that use the same training algorithm. However, these models are not trained on the entire training set. Instead, each model is trained on a different subset of the data. This aims to improve the stability and accuracy of a machine learning model by reducing variance and preventing overfitting.\n",
    "\n",
    "Bagging stands for Boostrap Aggregation. Its name refers to the fact that it uses a technique known as __bootstrap__. Consider the case where you have 3 balls labeled A, B, and C. A bootstrap sample is a sample drawn with replacement. In the training phase, bagging consists of N different boostrap samples from the training set. Each of these bootstrap samples are then used to train N models that use the same algorithm. When a new instance is fed to the different models forming the bagging ensemble, each model output its prediction. The meta-model collects these predictions and outputs a final prediction depending on the nature of the problem. In classification, the final prediction is obtained by majority voting. The corresponding classifier in scikit-learn is _BaggingClassifier_. In regression, the final prediction is the average of the predictions made by the individual models forming the ensemble. The corresponding regressor in scikit-learn is _BaggingRegressor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988cd60d-7bde-4615-a763-e2581d85521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "285c1f00-6908-4081-897b-7e8144d95539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(estimator=dt, n_estimators=50, random_state=1)\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ecdae-3bf6-4da7-86a2-5720e3ef31b6",
   "metadata": {},
   "source": [
    "## Out of Bag Evaluation\n",
    "\n",
    "Recall that in bagging, some instances may be sampled several times for one model. On the other hand, other instances may not be sampled at all. On average, for each model, 63% of the training instances are sampled. The remaining 37% that are not sampled constitute what is known as __Out-of-bag__ (OOB) instances. Since OOB instances are not seen by the model during training, these can be used to estimate the performance of the ensemble without the need for cross-validation. This technique is known as __OOB Evaluation__. \n",
    "\n",
    "__Case Study__ \\\n",
    "In the following exercises, you'll compare the OOB accuracy to the test set accuracy of a bagging classifier trained on the Indian Liver Patient dataset. In sklearn, you can evaluate the OOB accuracy of an ensemble classifier by setting the parameter oob_score to True during instantiation. After training the classifier, the OOB accuracy can be obtained by accessing the .oob_score_ attribute from the corresponding instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e87b30-5976-4e56-8f07-3ba27be827fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.690, OOB accuracy: 0.676\n"
     ]
    }
   ],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)\n",
    "\n",
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba59fd-cea9-4549-bb78-91155223e07a",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "In bagging, the base estimator could be any model including the decision-tree, logistic regression, neural network. Each estimator is trained on a distinct bootstrap sample drawn from the training set, using all available features. Random forest is an ensemble method that uses a desicion tree as a base estimator. In Random Forests, each estimator is trained on a different bootstrap sample having the same size as the training set. Random forests introduces further randomization than bagging when training each of the base estimators. When each tree is trained, only d features can be sampled at each node without replacement, where d is a number smaller than the total number of features. The node is then split using the sampled feature that maximizes information gain. In scikit-learn, d defaults to the square root of the number of features. For example, if there are 100 features, only 10 features are sampled at each node. Once trained, predictons can be made on new instances. When a new instance is fed to the different base estimators, each of them outputs a prediction. The predictions are then collected by the random forests meta-model and a final prediction is made depending on the nature of the problem. For classification, the final prediction is made by majority voting. The corresponding scikit-learn class is _RandomForestClassifier_. For regression, the final prediction is the average of all the labels predicted by the base estimators. The corresponding scikit-learn class is _RandomForestRegressor_. In general, random forests achieve a lower variance than individual trees.\n",
    "\n",
    "__Feature IMportance__ \\\n",
    "When a tree-based  method is trained, the predictive power of a feature or its importance can be assessed. In scikit-learn, feature importance is assessed by measuring the tree nodes use a particular feature to reduce impurity. Note that the importance of a feature is expressed as a percentage, indicating the weight of that feature in training and prediction. The feature importances can be accessed by extracting the _feature_importance__ attribute from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adcd72a-c662-4d78-aaec-0bb3790eb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\bikes.csv\")\n",
    "\n",
    "X = bikes.drop('cnt', axis=1)\n",
    "y = bikes['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53dd30b8-5afb-4449-ad0f-00773b791bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 54.49\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train) \n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = (MSE(y_test, y_pred)) ** (1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c905e374-88c7-469c-8a69-2e22c78c904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 112.99\n"
     ]
    }
   ],
   "source": [
    "## training a single decision tree\n",
    "\n",
    "single_tree = DecisionTreeClassifier(random_state=2)\n",
    "single_tree.fit(X_train, y_train)\n",
    "pred_tree = single_tree.predict(X_test)\n",
    "rmse_tree = (MSE(y_test, pred_tree)) ** (1/2)\n",
    "\n",
    "# Print rmse_tree\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f7f6ec-9c1c-4994-9d39-59b2d391c723",
   "metadata": {},
   "source": [
    "The test set RMSE achieved by rf is significantly smaller than that achieved by a single Decision-Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb8bd92-d307-4400-9610-019887ac9d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizing features importances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6435d2-8eba-417b-825b-f1866e92af30",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting refers to an ensemble in which many predictors (models) are trained, and each predictor learns from the errors of its predecessor. In boosting, several weak learners are combined to form a strong learner. A weak learner is a model doing slightly better than random guessing. For example, a decision tree with a maximum-depth of one, known as a __decision stump__, is a weak learner. In boosting, an ensemble of predictors are trained sequentially, and each predictor tries to correct the errors made by its predecessors.\n",
    "\n",
    "The two most popular boosting methods are:\n",
    "1. Adaboost\n",
    "2. Gradient boosting\n",
    "\n",
    "#### Adaboost\n",
    "\n",
    "Adaboost stands for __Adaptive Boosting__. In Adaboost, each predictor pays attention to the instances wrongly predicted by its predecessor by constantly changing the weights of the training instances. Furthermore, each predictor is assigned a coefficient , $\\alpha$, that weighs its contribution in the ensemble's final prediction. $\\alpha$ depends on the predictor's training error. Lets asssume there are N predictors in total. First, predictor1 is trained on the initial dataset (X,y), and the training error for predictor1 is determined. This error can then be used to determine $\\alpha_1$ which is predictor1's coefficient. $\\alpha_1$ is then used to determine the weights W(2) of the training instances for predictor2. As a result, the incorrectly predicted instances acquire higher weights. When the weighted instances are used to train predictor2, this predictor is forced to pay more attention to the incorrectly predicted instances. This process is repeated sequentially, until the N predictors forming the ensemble are trained.\n",
    "\n",
    "__Learning rate__ \\\n",
    "An important parameter used in training is the learning rate, $\\eta$. $\\eta$ is a number between 0 and 1; it is used to shrink the coefficient alpha of a trained predictor. It's important to note that there's a trade-off between eta and the number of estimators. A smaller value of $\\eta$ should be compensated by a greater number of estimators.\n",
    "\n",
    "__Prediction__ \\\n",
    "Once all the predictors in the ensemble are trained, the label of a new instance can be predicted depending on the nature of the problem. For classification, each predictor predicts the label of the new instance and the ensemble's prediction is obtained by weighted majority voting. The corresponding class in scikit-learn is _AdaBoostClassifier_. For regression, the same procedure is applied and the ensemble's prediction is obtained by performing a weighted average. The corresponding class in scikit-learn is _AdaBoostRegressor_. It's important to note that individual predictors need not to be CARTs. However CARTs are used most of the time in boosting because of their high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5733c66-d852-4729-88e0-4a9fa4a645ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = indian_liver_patient.drop(\"Liver_disease\", axis=1)\n",
    "y = indian_liver_patient['Liver_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec3903e3-7037-4b54-891d-b4e5b5cbf66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Odinaka Ekemezie\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(estimator=dt, n_estimators=180, random_state=1)\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd2edc-dc7b-4675-94d0-c7762eb7b2a6",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "In gradient boosting, each predictor in the ensemble corrects its predecessor's error. In contrast to Adaboost, the weights of the training instances are not tweaked. Instead, each predictor is trained using the residual errors of its predecessor as labels. Let's further explain gradient boosting with the analogy below: \n",
    "* The Goal: To create a perfectly accurate portrait of a person.\n",
    "\n",
    "* Artist 1 (The First Weak Learner): Makes a very simple, rough initial sketch of the person's face. It's a bad prediction. The \"error\" (the residual) is the difference between this rough sketch and the actual person's face.\n",
    "\n",
    "* The \"Boost\": The team leader doesn't show the original photo to the second artist. Instead, they show them a picture of the errora ghostly image highlighting where the first sketch was wrong (e.g., \"the nose is too short,\" \"the chin is too wide\").\n",
    "\n",
    "* Artist 2: This artist's only job is to create a drawing of just those corrections.\n",
    "\n",
    "* The Final Portrait: The final, accurate portrait is created by taking the first rough sketch, then adding the corrections from Artist 2, then adding the corrections from Artist 3, and so on. Each artist makes a small, incremental improvement, gradually turning the rough sketch into a masterpiece.\n",
    "\n",
    "Now lets asssume there are N trees in an ensemble. Tree1 is trained using the features matrix X and the dataset labels y. The predictions labeled $\\hat{y_1}$ are used to determine the training set residual errors r1. Tree2 is then trained using the features matrix X and the residual errors r1 of Tree1 as labels. The predicted residuals $\\hat{r_1}$  are then used to determine the residuals of residuals which are labeled r2. This process is repeated until all of the N trees forming the ensemble are trained.\n",
    "\n",
    "__Shrinkage__ \\\n",
    "An important parameter used in training gradient boosted trees is shrinkage. In this context, shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate $\\eta$ which is a number between 0 and 1. Similarly to AdaBoost, there's a trade-off between $\\eta$  and the number of estimators. Decreasing the learning rate needs to be compensated by increasing the number of estimators in order for the ensemble to reach a certain performance.\n",
    "\n",
    "__Prediction__ \\\n",
    "Once all trees in the ensemble are trained, prediction can be made. When a new instance is available, each tree predicts a label and the final ensemble prediction is given by the formula shown below:\n",
    "$$\n",
    "y_{pred} = y_1 + \\eta{r_1} + ... + \\eta{r_N}\n",
    "$$\n",
    "In scikit-learn, the class for a gradient boosting regressor is _GradientBoostingRegressor_. For classification problems, the class implementing gradient-boosted-classification in scikit-learn is _GradientBoostingClassifier_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d12d96-9c91-4860-8562-234bd787badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\bikes.csv\")\n",
    "\n",
    "X = bikes.drop('cnt', axis=1)\n",
    "y = bikes['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58814a70-2dc9-4959-9952-4341b75b8b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 49.537\n"
     ]
    }
   ],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(n_estimators=200, \n",
    "            max_depth=4,\n",
    "            random_state=2)\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test ** (1/2)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0c3a5-e410-4c6e-ad68-41986066a445",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting\n",
    "\n",
    "Gradient boosting involves an exhaustive search procedure. Each tree in the ensemble is trained to find the best split-points and the best features. This procedure may lead to CARTs that use the same split-points and possibly the same features. To mitigate these effects, you can use an algorithm known as __stochastic gradient boosting__. Stochastic Gradient Boosting (SGB) is a modification of the Gradient Boosting method that incorporates randomness to improve its performance and reduce overfitting. The \"stochastic\" part simply means random. It combines the error-correcting, sequential nature of Gradient Boosting with the random sampling idea from Bagging. In stochastic gradient boosting, each CART is trained on a random subset of the training data. This subset is sampled without replacement. Furthermore, at the level of each node, features are sampled without replacement when choosing the best split-points. As a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees.\n",
    "\n",
    "Lets describe the training procedure of Stochastic Gradient Boosting. First, instead of providing all the training instances to a tree, only a fraction of these instances are provided through sampling without replacement. The sampled data is then used for training a tree. However, not all features are considered when a split is made. Instead, only a certain randomly sampled fraction of these features are used for this purpose. Once a tree is trained, predictions are made and the residual errors can be computed. These residual errors are multiplied by the learning rate $\\eta$ and are fed to the next tree in the ensemble. This procedure is repeated sequentially until all the trees in the ensemble are trained. The prediction procedure for a new instance in stochastic gradient boosting is similar to that of gradient boosting.\n",
    "\n",
    "In scikit-learn, Stochastic Gradient Boosting isn't a separate model. It's implemented directly within GradientBoostingRegressor and GradientBoostingClassifier using the _subsample_ hyperparameter. The subsample is set so each tree samples a particular percentage of the data. The parameter _max_features_ can also be set so that each tree uses a particular percentage of available features to perform the best-split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c51bd01e-012e-4502-b3e0-a8a42f003f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 47.260\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,\n",
    "            random_state=2)\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test ** (1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083db867-a17e-4464-a369-f8ec66f2a788",
   "metadata": {},
   "source": [
    "The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor (which was 49.537)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd07ba-3e6f-4d76-be29-a0c879153eb9",
   "metadata": {},
   "source": [
    "## Tuning a CART's Hyperparameters\n",
    "\n",
    "To obtain a better performance, the hyperparameters of a machine learning mpdel should be tuned. Machine learning models are characterized by parameters and hyperparameters. Parameters are learned from data through training; examples of parameters include the split-feature and the split-point of a node in a CART. Hyperparameters are not learned from data; they should be set prior to training. Examples of hyperparameters include the maximum-depth, min_samples_leaf, and the splitting-criterion of a CART.\n",
    "\n",
    "__Hyperparameter tuning__ \\\n",
    "Hyperparameter tuning consists of searching for the set of optimal hyperparameters for the learning algorithm. The solution involves finding the set of optimal hyperparameters that yields an optimal model. The optimal model yields an optimal score. The score function measures the agreement between true labels and a model's predictions. In sklearn, it defaults to accuracy for classifiers and r-squared for regressors. A model's generalization performance is evaluated using __cross-validation__.\n",
    "\n",
    "A legitimate question that you may ask is: why bother tuning hyperparameters? Well, in scikit-learn, a model's default hyperparameters are not optimal for all problems. Hyperparameters should be tuned to obtain the best model performance.\n",
    "\n",
    "There are many approaches for hyperparameter tuning including: grid-search, random-search, bayeisian optimization, genetic algorithms, and so on. Here, we'll only be exploring the method of grid-search.\n",
    "\n",
    "__Grid-search Cross Validation__ \\\n",
    "In grid-search cross-validation, first you manually set a grid of discrete hyperparameter values. Then, you pick a metric for scoring model performance and you search exhaustively through the grid. For each set of hyperparameters, you evaluate each model's score. The optimal hyperparameters are those for which the model achieves the best cross-validation score. Note that grid-search suffers from the curse of dimensionality, i.e., the bigger the grid, the longer it takes to find the solution.\n",
    "\n",
    "Let's walk through a concrete example to understand this procedure. Consider the case of a CART where you search through the two-dimensional hyperparameter grid shown here: \\\n",
    "max__depth = {2,3,4} \\\n",
    "min_samples_leaf = {0.05, 0.1} \\\n",
    "The dimensions correspond to the CART's maximum-depth and the minimum-percentage of samples per leaf. For each combination of hyperparameters, the cross-validation score is evaluated using k-fold CV for example. Finally, the optimal hyperparameters correspond to the model achieving the best cross-validation score.\n",
    "\n",
    "To inspect hyperparameters in scikit-learn, first instantiate a DecisionTreeClassifier. call ._get_params()_  method to print out a dictionary where the keys are the hyperparameter names like max_depth, max_features and min_samples_leaf. Note that max_features is the number of features to consider when looking for the best split. When it's a float, it is interpreted as a percentage. You can learn more about these hyperparameters by consulting scikit-learn's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "156a0bac-ea3c-40c0-abad-b63de035236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = indian_liver_patient.drop(\"Liver_disease\", axis=1)\n",
    "y = indian_liver_patient['Liver_disease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecef6988-4a04-4a1a-9bbd-ddc7aadc7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a66eed8-3c0a-45a6-b420-553b4bf497d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# get the hyperparameter names\n",
    "print(dt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2a594b3-911c-40ed-8658-86670552aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.731\n"
     ]
    }
   ],
   "source": [
    "# Define params_dt\n",
    "params_dt = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'min_samples_leaf': [0.12,0.14,0.16,0.18]\n",
    "}\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afc70b83-27c8-43f7-a1d5-332c178de080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      " {'max_depth': 3, 'min_samples_leaf': 0.14}\n"
     ]
    }
   ],
   "source": [
    "# extract the best hyperparameters\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyperparameters:\\n', best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd30d173-7849-4c41-a21a-95805b102e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV roc-auc score:\n",
      " 0.717498076826435\n"
     ]
    }
   ],
   "source": [
    "# Extract best CV score from grid_dt\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV roc-auc score:\\n', best_CV_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cfd8b-3d48-4615-9cb6-05bee4531895",
   "metadata": {},
   "source": [
    "## Tuning Random Forest's Hyperparameters\n",
    "\n",
    "In addition to the hyperparameters of the CARTs forming random forests, the ensemble itself is characterized by other hyperparameters such as the number of estimators, whether it uses bootstraping or not and so on. As a note, hyperparameter tuning is computationally expensive and may sometimes lead only to very slight improvement of a model's performance. For this reason, it is desired to weigh the impact of tuning on the pipeline of your data analysis project as a whole in order to understand if it is worth pursuing.\n",
    "\n",
    "To inspect the hyperparameters of a RandomForestRegressor, first, import RandomForestRegressor from sklearn.ensemble and then instantiate a RandomForestRegressor, rf. The hyperparameters of rf along with their default values can be accessed by calling rf's _.get_params()_ method. In the following, we'll be optimizing n_estimators, max_depth, min_samples_leaf and max_features. You can learn more about these hyperparameters by consulting scikit-learn's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccaefbd3-b039-4c45-9036-88df9537e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv(r\"C:\\Users\\Odinaka Ekemezie\\Downloads\\bikes.csv\")\n",
    "\n",
    "X = bikes.drop('cnt', axis=1)\n",
    "y = bikes['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02a6aad9-1d6d-4c55-b388-144d9cc58137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 2, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "rf = RandomForestRegressor(random_state=2)\n",
    "\n",
    "# extract the hyperparameter names and their default values\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2ab9636-0c77-471d-8e3f-f8181a416aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Test RMSE of best model: 89.583\n"
     ]
    }
   ],
   "source": [
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {\n",
    "    'n_estimators': [100,350,500],\n",
    "    'max_features': ['log2', 'sqrt'],\n",
    "    'min_samples_leaf': [2,10,30]\n",
    "}\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "# fit the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = (MSE(y_test, y_pred)) ** (1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ef2ec65-8119-4f10-8708-2a69fee681c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_features='log2', min_samples_leaf=2, n_estimators=500,\n",
      "                      random_state=2)\n"
     ]
    }
   ],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb9203-1a35-492a-91c0-4bb3cce03bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
